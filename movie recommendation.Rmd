---
title: "Movie Recommendation"
output: html_notebook
---

We will use the following libraries for this project:

```{r}
library(tidyverse) # for tidy data
library(gridExtra) # to plot graphs in a grid, or next to each other
library(dplyr) # for data manipulation
library(ggplot2) #for visualisations
library(caret) # for ML, resampling and model training

library(dslabs) # for the movielens data
data("movielens") # load-up the movielens data
```

## Examination & Preparation

Next, carry out preliminary examination of the data:

```{r}
dim(movielens)
```

This dataset has 100,004 rows and 7 columns.

```{r}
head(movielens)
```

The table is in a tidy format with each row representing a rating given by one user to one movie. 

Lets check how many users have provided ratings and how many movies were rated:

```{r}
#create a table summarising the total numbers of users and movies. Use the n_distinct() function and not count() as the latter will only give the number of times that user has given a rating.
movielens %>% summarise(n_users = n_distinct(userId),
                        n_movies = n_distinct(movieId))
```
So there are 671 users and 9066 movies. This does not add-up. 671 x 9066 = 6,083,286. Yet there are only 100,004 rows. The only explanation is that not every user rated every movie. 

Since that is the case, if we imagine a matrix where rows are users and columns are movies (or vice versa) there would be a lot of empty cells. 

The gather() function, or alternatively pivot_wider() function allows us to convert to this format, but if we try it for the entire 671x9066 matrix, it will crash R. So let's look at the matrix for 7 users and top 5 most rated movies:

```{r}
#to identify the top 5 movies first use count() function to count unique number of movies using their Id numbers, and then select the top 5 by using the top_n() function. Both functions are from the dplyr package
keep <- movielens %>% 
  count(movieId) %>%
  top_n(5) %>%
  pull(movieId)

#choose 7 random users, in this case lets say users from 13 to 20, and create a tidy matrix

tab <- movielens %>%
  filter(userId %in% c(13:20)) %>%
  filter(movieId %in% keep) %>%
  select(userId, title, rating) %>%
  #spread(title, rating)
  pivot_wider(names_from = title, 
              values_from = rating) #if we don't use pivot_wider() function (this function supersedes spread() function that was previously used) from the tidyr package the resulting matrix will have the same output format as movielens - ie, each row represents one rating given by one user to one movie. This means, without pivot_wider, rows would list users and their ratings for each movie before doing the same for the next user and so on. 
tab
```

As can be seen there are NAs present.

The task of recommendation system can be thought of as filling in the NAs in this matrix. We can also visualise where NAs are in the matrix. Lets try a visualisation for a random sample of 100 movies and 100 users:

```{r}
#randomly select 100 users

users <- sample(unique(movielens$userId), 100)

movielens %>%
  filter(userId %in% users) %>%
  select (userId, movieId, rating) %>%
  mutate(rating = 1) %>% #changes all the assigned ratings to 1. Choice of 1 is arbitrary so long as long as they are uniform so that the image can show the contrast between NAs and those that received a rating of any value.
  pivot_wider(names_from = movieId, values_from = rating) %>%
  select(sample(ncol(.), 100)) %>%
  as.matrix() %>%
  t(.) %>% # this transposes the matrix so that movieIds are the rows and users are the columns
  image(1:100, 1:100, . , xlab = "Movies", ylab = "Users")
```


Now we can start trying to make some predictions.

There are some complications that need to be expressed from the outset. Note that each outcome Y has a different set of predictors. In other words, to predict rating for movie i by user u, in principle we should be able to use as predictors all other ratings related to movie i and by user u. However, the complicating issue is that different users rate different movies, and they rate different number of movies. It may also be the case that we may be able to see info from other movies that we determine to be similar to movie i, or from users determined to be similar to user u. In essence, the entire matrix can be used as predictors for each cell.

Lets look at some of the general properties of the data to better understand the challenges.

We would expect some movies to get rated a lot more than some others - think of a blockbuster vs some obscure movie. We would also expect some users to be more active in rating movies than others. We can check these with histograms:

```{r}
# Histogram for movies assigned to p1
p1 <- movielens %>%
  count(movieId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30,
                 colour = "black") +
  scale_x_log10() +
  ggtitle("Movies")

#Histogram for users assigned to p2
p2 <- movielens %>%
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30,
                 colour = "black") +
  scale_x_log10() +
  ggtitle("Users")

#arrange p1 and p2 in grid formation
grid.arrange(p1, p2, ncol = 2)
```

Our expectations are confirmed - some movies receive a lot more ratings, and some users are a lot more active than the others.

Our main goal is to build an algorithm with data we have collected that will then be applied outside our control - movie recommendations for users.

As a 1st step, lets create a test set to assess the accuracy of the models we implement.

```{r}
set.seed(755) #for reproduceability

#1st step:use createDataPartition from caret package to partition the ratings
test_index <- createDataPartition(y = movielens$rating,
                                  times = 1,
                                  p = 0.2,
                                  list = FALSE)

#2nd step: assign the partitioned data to training and test sets
train_set <- movielens[-test_index, ]
test_set <- movielens[test_index, ]
```

If we leave it like this, then there is a chance that some users and movies that are in the training set are not in the test set and vice versa. Since that would not be helpful for our predictions, we need to remove those entries using the semi_join function:

```{r}
#semi_join(x,y) function from dplyr package returns all rows from x with a match in y
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

```


### Definition of Success: Loss Function
To compare different models or to see how we are doing compared to some baseline, we need to quantify what it means to do well. So we need a loss function.

The Netflix challenge used the typical error loss: they decided on a winner based on the residual mean squared error (RMSE) on a test set. We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{u,i}$. The RMSE is then defined as: 

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$

with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.

Remember that we can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good.

Let's write a function that computes the RMSE for vectors of ratings and their corresponding predictors:

```{r}
#function to compare models' RMSEs using the above formula

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

Now we are ready to build models & compare them to each other.





## Models

The Netflix challenge winners implemented two general classes of models.
--> One similar to kNN
--> Matrix factorisation (main focus of this project)


### 1st Model: Establishing the base to compare models against

Let's start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:


$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

with $\varepsilon_{i,u}$ independent errors sampled from the same distribution centered at 0 and $\mu$ the "true" rating for all movies. We know that the estimate that minimizes the RMSE is the least squares estimate of $\mu$ and, in this case, is the average of all ratings:

```{r}
#set average of all ratings as our estimate of mu
rating_avg <- mean(train_set$rating)
rating_avg
```

So we would predict an average rating of 3.54 for all movies regardless of user. If this is our prediction, then our RMSE for this prediction would be:

```{r}
#use the RMSE function we just created above and assign it to naive_RMSE
naive_RMSE <- RMSE(test_set$rating, rating_avg)
naive_RMSE
```

The RMSE of our naive model is about 1.05.From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE of about 1. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do better than 1.05. 

As we go along, we will be comparing different approaches. Let's start by creating a results table with this naive approach:

```{r}
#we can use tibble for creating the results table

rmse_results <- tibble("Naive: Just the Average", naive_RMSE) %>% 
  set_names(c("method", "RMSE")) #set names add column names. This is needed for us to use bind_rows() function later on to add more rows with results from our other models
rmse_results
```


### 2nd Model: Adding movie effects

We know from above and from data that some movies are generally rated higher than others (blockbuster vs obscure). To incorporate this into our model we add the term b_i to represent average ranking for movie i.

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

Statistics textbooks refer to the $b$s as effects. However, in the Netflix challenge papers, they refer to them as "bias", thus the $b$ notation.

We can use least squares to estimate the $b_i$ with lm(rating ~ as.factor(movieId), data = movielens) but this function would be very slow because there are thousands of $b_i$ since each movie gets one. 

Instead, in this particular situation, we know that the least squares estimate $\hat{b}_i$ is just the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$.
$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$
$$
b_i = Y_{u,i} - \mu + \varepsilon_{u,i}  
$$

So we can compute them this way

```{r}
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - rating_avg))
head(movie_avgs)
```

We can plot movie averages to see the variation in estimates:

```{r}
qplot(b_i, data = movie_avgs, bins = 10, colour = I("black"))
```

Remember our rating average is 3.5 as calculated above ($\hat{\mu}=3.5$), so a $b_i = 1.5$ implies a perfect five star rating, and a $b_i = -3$ implies a rating of half a star.

Let's see how much our prediction improves once we use $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i$:

```{r}
#since formula is mu + bi estimates, we will add test_set's bi to rating average 
predicted_ratings <- rating_avg + test_set %>% #however, test_set does not have b_i. 
  left_join(movie_avgs, by="movieId") %>% #So we will use left_join(x,y) function to add b_is we calculated in training set by matching their movieId- ie average of rating differences to the mean rating of 3.5 by each movie.
  .$b_i #alternatively pull(b_i)

#we can check the performance of this model using our RMSE function:
RMSE(predicted_ratings, test_set$rating)
```

We can see that we have improved our accuracy from 1.05 to just below 0.99. Lets add this to our results table:

```{r}
movie_effect_model_rmse <- RMSE(predicted_ratings, test_set$rating)

#we will use bind_rows() from dplyr package to add the movie effect model's RMSE to our table. This function binds two data frames by rows. For it to work, the columns of rmse_results need names which can be done by set_names function (see above where table is initiall created).
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie Effect Model",
                                 RMSE = movie_effect_model_rmse)) 
rmse_results
```

This improvement was due to incorporating movie effects. Lets see if we can similarly incorporate user effects, and whether that would further improve our prediction.




### 3rd Model: Adding user effects

User effect refers to the idea that different users have different tastes and so they differ in how they rate movies.

To see if our assumption holds by the data, we should explore the data by computing the average rating for user $u$, and lets limit it to those that have rated at least 100 movies.

```{r}
#create histogram of user effect

train_set %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarise(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, colour = "black")
```

We can see that some users that rated at least 100 movies are a bit cranky with low average ratings, and some love almost every movie. So there is a lot of variability among the users. 

This implies that a further improvement to our model may be:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

where $b_u$ is a user-specific effect. Now if a cranky user (negative $b_u$) rates a great movie (positive $b_i$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5. 

To fit this model, we could again use lm(rating ~ as.factor(movieId) + as.factor(userId)) but for computational time reasons we won't. 

Instead, we will compute an approximation by computing $\hat{\mu}$ and $\hat{b}_i$ and estimating $\hat{b}_u$ as the average of 
$$
y_{u,i} - \hat{\mu} - \hat{b}_i
$$

```{r}
user_avgs <- train_set %>%
  left_join(movie_avgs, by= "movieId") %>% #adding movie averages to the training set
  group_by(userId) %>% 
  summarise(b_u = mean(rating - rating_avg - b_i))
head(user_avgs)
```

We can now construct predictors and see how much the RMSE improves:

```{r}
predicted_ratings <- test_set %>%
  left_join(movie_avgs, by="movieId") %>%
  left_join(user_avgs, by="userId") %>%
  mutate(pred = rating_avg + b_i + b_u) %>%
  .$pred #or pull(pred)

#calculate the RMSE
movie_and_user_effect_rmse <- RMSE(test_set$rating, predicted_ratings)

#add the rmse to the results table
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie & User Effects Model",
                                 RMSE = movie_and_user_effect_rmse))
rmse_results
```

We see an improved RMSE at about 0.905 now. Our only movie effect model had about 6% improvement (from 1.05 to 0.988). We should pause and check where we made mistakes. We can first check the top 10 mistakes we made with movie effect, $b_i$, only. The mistakes can be identified by look at the residuals which is equal to the difference between actual rating in test set and our prediction ($mu_hat$ + $b_i$)

```{r}
test_set %>%
  left_join(movie_avgs, by="movieId") %>%
  mutate(residual = rating - (rating_avg + b_i)) %>%
  arrange(desc(abs(residual))) %>% #although arrange(residual) gives the same answer, note the logic of it. residuals can be in either direction, so taking the absolute values and then sorting them in descending order is the logic behind this arrangement.
  slice(1:10)
```

There are some obscure and some known movies in the top10 mistakes list. It may be that there are not many ratings for these so our $b_i$ may be error prone. We can look at the top 10 best and worst movies based only on $\hat{b}_i$ and see how many times these movies have been rated. To do so, First, let's create a database that connects `movieId` to movie title

```{r}
movie_titles <- movielens %>%
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate:

```{r}
movie_avgs %>% 
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>%
  slice(1:10) %>%
  .$title
```

and here are the 10 worst:

```{r}
movie_avgs %>% 
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>%
  slice(1:10) %>%
  .$title
```

They all seem quite obscure. let's see how often they have been rated actually

```{r}
train_set %>%
  count(movieId) %>%
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>%
  slice(1:10) %>%
  pull(n)
```

These movies we estimated to be the top 10 best and worst movies were actually rated by very few users, in most cases just 1. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of $b_i$, negative or positive, are more likely.

These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

Usually, when we are accounting for uncertainty we would usually use confidence intervals. However, when making predictions, we need one number, one prediction, not an interval. For this, we introduce the concept of regularization.





## Regularisation

Regularisation permits us to penalize large estimates that are formed using small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions. It was one of the techniques that was used by the winners of Netflix Challenge.

The general idea behind regularization is to constrain the total variability of the effect sizes. We are just adding a penalty for large values of $b$ to the sum of squares equations that we want to minimise. Specifically, instead of minimising the least squares equation, we minimize an equation that adds a penalty.

$$ 
\sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2 
$$
The first term is just the sum of squares and the second is a penalty that gets larger when many $b_i$ are large. Using calculus we can actually show that the values of $b_i$ that minimize this equation are:

$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $n_i$ is the number of ratings made for movie $i$. This approach will have our desired effect: when our sample size $n_i$ is very large, a case which will give us a stable estimate, then the penalty $\lambda$ is effectively ignored since $n_i+\lambda \approx n_i$. However, when the $n_i$ is small, then the estimate $\hat{b}_i(\lambda)$ is shrunken towards 0. The larger $\lambda$, the more we shrink.

Let's compute these regularised estimates of $b_i$ using $\lambda=3$. Later, we will see why we picked 3.

```{r}
lambda <- 3
movie_reg_avgs <- train_set %>%
  group_by(movieId) %>%
  summarise(b_i = sum(rating - rating_avg) / (n() + lambda), 
            n_i = n())
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates. The sizes of the circlues in the graph tells us how large $n_i$ was:

```{r}
tibble(original = movie_avgs$b_i,
       regularised = movie_reg_avgs$b_i,
       n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularised, size = sqrt(n))) +
  geom_point(shape = 1, alpha = 0.5)
```

From the plot it can be seen that as n gets smaller, the values tend towards 0.

Now lets look at the top 10 best movies based on the penalised estimates $\hat{b}_i(\lambda)$:

```{r}
train_set %>% 
  count(movieId) %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>%
  slice(1:10) %>%
  pull(title)
```

These make much more sense! These movies are watched more and have more ratings. Here are the top 10 worst movies:

```{r}
train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  pull(title)
```

We can now check if we improved our results:

```{r}
head(movie_reg_avgs)
predicted_ratings <- test_set %>%
  left_join(movie_reg_avgs, by="movieId") %>%
  mutate(pred = rating_avg + b_i) %>%
  pull(pred)

#apply the RMSE function
reg_movie_effect_rmse <- RMSE(predicted_ratings, test_set$rating)
reg_movie_effect_rmse

#add the results to our table
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Regularised Movie Effect Model",
                                 RMSE = reg_movie_effect_rmse))
rmse_results
```

We see that regularisation has improved movie effect RMSE from 0.988 to 0.97. 

Of course, we set lambda to 3 above. However, lambda is a tuning parameter, so we can use cross validation to choose the best penalty to apply:

```{r}
#potential lambdas from 0 to 10 in 0.25 increments
lambdas <- seq(0, 10, 0.25)

just_the_sum <- train_set %>%
  group_by(movieId) %>%
  summarise(s = sum(rating - rating_avg), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>%
    left_join(just_the_sum, by="movieId") %>%
    mutate(b_i = s / (n_i + l)) %>%
    mutate(pred = rating_avg + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)
lambdas[which.min(rmses)]
```

lambda = 3 is what minimises RMSE, this is why we chose 3 at the beginning. However, while we show this as an illustration, in practice we should be using full cross-validation just on the train set, without using the test set until the final assessment. The test set should NEVER be used for tuning.


We can use regularization for the estimate user effects as well. We are minimizing:

$$
\sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

The estimates that minimize this full model can be found similarly to what we did above. Here we use cross-validation to pick a $\lambda$:

```{r}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - rating_avg) / (n() + l))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - rating_avg - b_i) / (n() + l))
  
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = rating_avg + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)
lambdas[which.min(rmses)]
```

lambda at 3.25 minimises the RMSE for the full model inclusive of user and movie effects.

We can now calculate the RMSE for this regularised full model which is the minimum of all the RMSEs:

```{r}
min(rmses)
```

So our model now reduced the prediction error to 0.88. We can add this to our table:

```{r}
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Regularised Movie & User Effect Model",
                                 RMSE = min(rmses)))
rmse_results
```


The models above leave out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well.

Matrix Factorisation can help with this.




## Matrix Factorisation

This is a widely used concept in ML. It is very much related to factor analysis, singular value decomposition (SVD), and principal component analysis (PCA). Here we describe the concept in the context of movie recommendation systems.

The model we have been using thus far $$ Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i} $$ accounts for user and movie effects, but leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals:

$$
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
$$

To see this, we will convert the data into a matrix so that each user gets a row, each movie gets a column, and $y_{u,i}$ is the entry in row $u$ and column $i$. For illustrative purposes, we will only consider a small subset of movies with many ratings and users that have rated many movies. We also keep Scent of a Woman (`movieId == 3252`) because we use it for a specific example:

```{r}
train_small <- movielens %>%
  group_by(movieId) %>%
  filter(n() >= 50 | movieId == 3252) %>%
  ungroup() %>%
  group_by(userId) %>%
  filter(n() >= 50) %>%
  ungroup()

y <- train_small %>%
  select(userId, movieId, rating) %>%
  pivot_wider(names_from = "movieId",
              values_from = "rating") %>%
  as.matrix()

dim(y)
```

To facilitate exploration, we add row names and column names:

```{r}
rownames(y) <- y[,1]
y <- y[,-1]

movie_titles <- movielens %>%
  select(movieId, title) %>%
  distinct()

colnames(y) <- with(movie_titles, 
                    title[match(colnames(y),
                                movieId)])

```

In order to convert them to residuals, we need to remove the column and row averages (effects):

```{r}
y <- sweep(y, 2, colMeans(y, na.rm = TRUE))
y <- sweep(y, 2, rowMeans(y, na.rm = TRUE))
```

If the model above explains all the signals, and the $\varepsilon$ are just noise, then the residuals for different movies should be independent from each other. To check we can look at some examples:

```{r}
m_1 <- "Godfather, The"
m_2 <- "Godfather: Part II, The"
p1 <- qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2)

m_3 <- "Goodfellas"
p2 <- qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3)

m_4 <- "You've Got Mail" 
m_5 <- "Sleepless in Seattle" 
p3 <- qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5)

gridExtra::grid.arrange(p1, p2 ,p3, ncol = 3)
```

We can see from the plots that the residuals are not independent but correlated. The first plot shows that users that liked The Godfather more than what the model expects them to, based on the movie and user effects, also liked The Godfather II more than expected. A similar relationship is seen when comparing The Godfather and Goodfellas. Although not as strong, there is still correlation. We see correlations between You've Got Mail and Sleepless in Seattle as well.

We can pairwise correlation to check:

```{r}
cor(y[, c(m_1, m_2, m_3, m_4, m_5)],
    use = "pairwise.complete")
```

We can see a positive correlation between gangster movies and positive correlation among romantic comedies. We also see a negative correlation between the two genres. 

The result tells us there is a structure in the data that the model doesn't account for. So how can we model this?

Here we use matrix factorisation.


### Factor Analysis
